{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 653,
   "id": "41ef5cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fd7347",
   "metadata": {},
   "source": [
    "READ CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "id": "1f3e67eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Artificial intelligence is transforming industries globally. Machine learning and deep learning are key componentsâ€¦ ', 'Robotics is advancing rapidly  integrating with AI for autonomous systems. Human-robot interaction is a growing fieldâ€¦ ', ' Data Engineering leverages tools and computation to move massive amounts of data. Big data analytics is crucialâ€¦ ']\n"
     ]
    }
   ],
   "source": [
    "articles_id = []\n",
    "titles = []\n",
    "contents = []\n",
    "\n",
    "with open(\"aritcles.csv\", \"r\") as a:\n",
    "    aritcles = csv.reader(a)\n",
    "\n",
    "    for row in aritcles:\n",
    "        if row[0] == \"id\":\n",
    "            continue\n",
    "        \n",
    "        articles_id.append(row[0])\n",
    "        titles.append(row[1])\n",
    "\n",
    "        # print(len(row))\n",
    "        content = \"\"\n",
    "        for i in range(2, len(row)):\n",
    "            content += row[i] + \" \"\n",
    "        \n",
    "        contents.append(content)\n",
    "\n",
    "    print(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3a3da2",
   "metadata": {},
   "source": [
    "CLEAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "id": "80fd05aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['artificial intelligence is transforming industries globally machine learning and deep learning are key components', 'robotics is advancing rapidly  integrating with ai for autonomous systems humanrobot interaction is a growing field', 'data engineering leverages tools and computation to move massive amounts of data big data analytics is crucial']\n"
     ]
    }
   ],
   "source": [
    "cleaned_contents = []\n",
    "\n",
    "for text in contents:\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"[^a-z ]\", \"\", text)\n",
    "    cleaned_contents.append(text.strip())\n",
    "\n",
    "print(cleaned_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d657cd",
   "metadata": {},
   "source": [
    "TOKENIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "id": "417ffcfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['artificial', 'intelligence', 'is', 'transforming', 'industries', 'globally', 'machine', 'learning', 'and', 'deep', 'learning', 'are', 'key', 'components'], ['robotics', 'is', 'advancing', 'rapidly', 'integrating', 'with', 'ai', 'for', 'autonomous', 'systems', 'humanrobot', 'interaction', 'is', 'a', 'growing', 'field'], ['data', 'engineering', 'leverages', 'tools', 'and', 'computation', 'to', 'move', 'massive', 'amounts', 'of', 'data', 'big', 'data', 'analytics', 'is', 'crucial']]\n"
     ]
    }
   ],
   "source": [
    "tokenized_articles = []\n",
    "for words in cleaned_contents:\n",
    "    tokenized_articles.append(words.split())\n",
    "\n",
    "print(tokenized_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34cdd2d",
   "metadata": {},
   "source": [
    "BAG OF WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "id": "46f94a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['artificial', 'intelligence', 'is', 'transforming', 'industries', 'globally', 'machine', 'learning', 'and', 'deep', 'are', 'key', 'components', 'robotics', 'advancing', 'rapidly', 'integrating', 'with', 'ai', 'for', 'autonomous', 'systems', 'humanrobot', 'interaction', 'a', 'growing', 'field', 'data', 'engineering', 'leverages', 'tools', 'computation', 'to', 'move', 'massive', 'amounts', 'of', 'big', 'analytics', 'crucial']\n"
     ]
    }
   ],
   "source": [
    "bag_of_words = []\n",
    "\n",
    "for article in tokenized_articles:\n",
    "    for word in article:\n",
    "        if word not in bag_of_words:\n",
    "            bag_of_words.append(word)\n",
    "\n",
    "print(bag_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1203b32a",
   "metadata": {},
   "source": [
    "TEXT to VECTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "id": "d16299fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def text_to_vector(article_words, the_bag_of_words):\n",
    "\n",
    "    vector = []\n",
    "    for word in the_bag_of_words:\n",
    "        if word in article_words:\n",
    "            vector.append(1)\n",
    "        else:\n",
    "            vector.append(0)\n",
    "    \n",
    "    return vector\n",
    "\n",
    "vectors = []\n",
    "for i in range(len(tokenized_articles)):\n",
    "    vectors.append( text_to_vector(tokenized_articles[i], bag_of_words) )\n",
    "\n",
    "print(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb3bb0e",
   "metadata": {},
   "source": [
    "COSINE SIMILARITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "id": "48872ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2307692307692308\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity(v1, v2):\n",
    "    dot_product = np.dot(v1, v2)\n",
    "    norm1 = np.linalg.norm(v1)\n",
    "    norm2 = np.linalg.norm(v2)\n",
    "\n",
    "    return dot_product / (norm1 * norm2)\n",
    "\n",
    "v1 = text_to_vector(contents[0], bag_of_words)\n",
    "v2 = text_to_vector(contents[1], bag_of_words)\n",
    "\n",
    "print( cosine_similarity(v1, v2) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1c41a1",
   "metadata": {},
   "source": [
    "SIMILARITY MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "id": "e9303afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.07161149 0.14322297]\n",
      " [0.07161149 1.         0.06666667]\n",
      " [0.14322297 0.06666667 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "similarity_matrix = []\n",
    "n = len(vectors)\n",
    "for i in range(n):\n",
    "\n",
    "    row = []\n",
    "    for j in range(n):\n",
    "        row.append( ( cosine_similarity(vectors[i], vectors[j]) ) )\n",
    "    \n",
    "    similarity_matrix.append(row)\n",
    "\n",
    "similarity_matrix = np.array(similarity_matrix)\n",
    "print(similarity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e37e69d",
   "metadata": {},
   "source": [
    "SAVE to PKL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "id": "b2aeaa13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.07161149 0.14322297]\n",
      " [0.07161149 1.         0.06666667]\n",
      " [0.14322297 0.06666667 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# WRITE \n",
    "with open(\"Similarity_Matrix.pkl\", \"wb\") as file:\n",
    "    pickle.dump(similarity_matrix, file)\n",
    "\n",
    "# READ\n",
    "with open(\"Similarity_Matrix.pkl\", \"rb\") as file:\n",
    "    print(pickle.load(file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff92ee7",
   "metadata": {},
   "source": [
    "MOST SIMILAR ARTICLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "id": "d3e4ad5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Most Similar Article to 'Data Engineering', ID: 3  is \n",
      "'The Rise of AI',  ID: 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def most_similar(article_id = 1):\n",
    "\n",
    "    index = articles_id.index( str(article_id) )\n",
    "    Similarities = similarity_matrix[index]\n",
    "    Similarities_sorted = sorted(Similarities, reverse=True)\n",
    "\n",
    "    best_article_score = Similarities_sorted[1]\n",
    "    best_article_index = np.where(similarity_matrix[index] == best_article_score)\n",
    "    best_article_id = articles_id[ best_article_index[0][0] ]\n",
    "    best_article = titles[best_article_index[0][0]]\n",
    "\n",
    "    print(f\"The Most Similar Article to '{titles[index]}', ID: {article_id}  is \\n'{best_article}',  ID: {best_article_id}\")\n",
    "\n",
    "\n",
    "most_similar(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
